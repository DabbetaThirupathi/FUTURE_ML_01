# -*- coding: utf-8 -*-
"""task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15qM9axmhELMkWcTuAkc_UHC6c6EZVRyh
"""

# Cell 1 — install required libraries
!pip install --quiet pandas numpy scikit-learn statsmodels
print("Packages installed.")

# Cell 2A — quick upload (files will be lost when runtime disconnects)
from google.colab import files
uploaded = files.upload()  # click "Choose Files" and upload Sample - Superstore.csv
print("Uploaded:", list(uploaded.keys()))

!ls -lh ml_task_1_refactor.py

from google.colab import files
uploaded = files.upload()  # choose "Sample - Superstore.csv"
print("Uploaded files:", list(uploaded.keys()))

!rm ml_task_1_refactor.py

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > ml_task_1_refactor.py <<'PY'
# import os
# import numpy as np
# import pandas as pd
# from sklearn.metrics import mean_absolute_error, mean_squared_error
# from statsmodels.tsa.statespace.sarimax import SARIMAX
# 
# def load_and_prepare(path: str):
#     df = pd.read_csv(path, encoding="ISO-8859-1")
#     df["Order Date"] = pd.to_datetime(df["Order Date"], format="%m/%d/%Y", errors="coerce")
#     df = df.dropna(subset=["Order Date"])
#     keep_cols = ["Order Date", "Sales", "Profit", "Region", "Category", "Sub-Category", "Product Name", "Segment"]
#     for c in keep_cols:
#         if c not in df.columns:
#             df[c] = np.nan
#     df = df[keep_cols].copy()
#     daily = (
#         df.groupby("Order Date", as_index=False)
#           .agg(Sales=("Sales", "sum"))
#           .rename(columns={"Order Date": "ds", "Sales": "y"})
#           .sort_values("ds")
#           .reset_index(drop=True)
#     )
#     daily["month"]   = daily["ds"].dt.month
#     daily["quarter"] = daily["ds"].dt.quarter
#     daily["is_holiday_spike"] = np.where(daily["ds"].dt.month.isin([11, 12]), 1, 0)
#     return df, daily
# 
# def train_sarimax(daily, horizon=180):
#     y = np.log1p(daily["y"].values)
#     exog = daily[["is_holiday_spike"]].values
#     model = SARIMAX(y, exog=exog, order=(1,1,1), seasonal_order=(1,1,1,7),
#                     enforce_stationarity=False, enforce_invertibility=False)
#     res = model.fit(disp=False)
#     fitted_log = res.fittedvalues
#     fitted = np.expm1(fitted_log)
# 
#     last_date = daily["ds"].max()
#     future_index = pd.date_range(last_date + pd.Timedelta(days=1), periods=horizon, freq="D")
#     future_exog = pd.DataFrame({"is_holiday_spike": np.where(future_index.month.isin([11, 12]), 1, 0)})
# 
#     fc = res.get_forecast(steps=horizon, exog=future_exog.values)
#     fc_mean  = np.expm1(fc.predicted_mean)
#     fc_int_raw = fc.conf_int(alpha=0.05)
#     fc_int_arr = np.expm1(np.asarray(fc_int_raw))
#     fc_lower = fc_int_arr[:, 0]
#     fc_upper = fc_int_arr[:, 1]
# 
#     future_df = pd.DataFrame({"ds": future_index, "yhat": fc_mean, "yhat_lower": fc_lower, "yhat_upper": fc_upper})
#     return fitted, future_df
# 
# def evaluate_holdout(daily, holdout_days=90):
#     split = len(daily) - holdout_days
#     if split < 10:
#         holdout_days = max(1, len(daily)//4)
#         split = len(daily) - holdout_days
#     train = daily.iloc[:split].copy()
#     test  = daily.iloc[split:].copy()
# 
#     y_train = np.log1p(train["y"].values)
#     exog_train = train[["is_holiday_spike"]].values
#     model = SARIMAX(y_train, exog=exog_train, order=(1,1,1), seasonal_order=(1,1,1,7),
#                     enforce_stationarity=False, enforce_invertibility=False)
#     res = model.fit(disp=False)
# 
#     exog_test = test[["is_holiday_spike"]].values
#     fc = res.get_forecast(steps=len(test), exog=exog_test)
#     yhat = np.expm1(fc.predicted_mean)
# 
#     mae  = mean_absolute_error(test["y"].values, yhat)
# 
#     # some sklearn versions don't accept `squared=` arg — compute MSE then sqrt for RMSE
#     mse = mean_squared_error(test["y"].values, yhat)
#     rmse = float(np.sqrt(mse))
# 
#     mape = (np.abs((test["y"].values - yhat) / np.clip(test["y"].values, 1e-8, None))).mean() * 100.0
#     return float(mae), float(rmse), float(mape)
# 
# def export_csvs(raw_df, daily, fitted_hist, future_df, metrics, outdir):
#     os.makedirs(outdir, exist_ok=True)
#     daily_out = daily.copy()
#     daily_out.to_csv(os.path.join(outdir, "historical_daily_sales.csv"), index=False)
#     hist = daily[["ds", "y"]].copy()
#     hist["yhat"] = fitted_hist
#     hist.to_csv(os.path.join(outdir, "historical_and_forecast_sales.csv"), index=False)
#     future_df.to_csv(os.path.join(outdir, "future_forecasted_sales.csv"), index=False)
#     metrics_df = pd.DataFrame({"Metric": list(metrics.keys()), "Value": [metrics["MAE"], metrics["RMSE"], metrics["MAPE"]]})
#     metrics_df.to_csv(os.path.join(outdir, "forecast_metrics.csv"), index=False)
# 
# if __name__ == "__main__":
#     import argparse
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--input", type=str, default="Sample - Superstore.csv")
#     parser.add_argument("--outdir", type=str, default="./exports")
#     parser.add_argument("--horizon", type=int, default=180)
#     args = parser.parse_args()
#     raw_df, daily = load_and_prepare(args.input)
#     fitted_hist, future_df = train_sarimax(daily, horizon=args.horizon)
#     mae, rmse, mape = evaluate_holdout(daily, holdout_days=90)
#     export_csvs(raw_df, daily, fitted_hist, future_df, {"MAE": mae, "RMSE": rmse, "MAPE": mape}, args.outdir)
#     print("CSV export complete to:", os.path.abspath(args.outdir))
# PY
#

!python ml_task_1_refactor.py --input "Sample - Superstore (1).csv" --outdir "./exports" --horizon 180

import glob, pandas as pd
files = sorted(glob.glob("exports/*.csv"))
print("Found files:", files)
for f in files:
    print("\n---", f, "---")
    print(pd.read_csv(f).head().to_string(index=False))

!zip -r exports.zip exports
from google.colab import files
files.download("exports.zip")

!zip -r exports.zip exports
from google.colab import files
files.download("exports.zip")

